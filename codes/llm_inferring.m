function S = llm_inferring(user_input, varargin)
% LLM_INFERRING  Get initial priors from an LLM (returns struct S and raw text).

% ---- Parse options
p = inputParser;
addParameter(p,'ModelName',"gpt-4o-mini");
addParameter(p,'Temperature',0.0);
addParameter(p,'MaxNumTokens',300);
addParameter(p,'UseAddon',true);
parse(p,varargin{:});
opt = p.Results;


% prompt = [
%     "You are an expert inferring initial controller priors from a short user preference." + ...
%     " Return STRICT JSON with keys: e_max, mu_0, sigma_0, bar_sigma, assumptions, rationale." + ...
%     " Meanings:" + ...
%     " - e_max: maximum lane tracking error tolerance. Larger for more aggressive turns/risk; smaller for more conservative/precise." + ...
%     " - mu_0: initial prior for road–tire friction (icy small, normal medium, dry large)." + ...
%     " - sigma_0: uncertainty (std^2) of the friction prior; larger if the user sounds unsure or contradictory." + ...
%     " - bar_sigma: confidence of the estimator on its measurements; increase if not sure if estimator is good." + ...
%     " Policy:" + ...
%     " - If the user uses vague words (""seems"", ""maybe"", ""not sure"", ""probably""), or does not sound certain, use sigma_0=0.3." + ...
%     " - Keep bar_sigma=0.3" + ...
%     " - Only change e_max with explicit user cues about aggressiveness or precision." + ...
%     " Valid discrete ranges:" + ...
%     " - e_max ∈ {3,5,10}; mu_0 ∈ {0.3,0.5,0.9}; sigma_0 ∈ {0.05,0.3}; bar_sigma ∈ {0.05,0.3}." + ...
%     " Output ONLY JSON in this exact shape: " + ...
%     " {""e_max"":0,""mu_0"":0.0,""sigma_0"":0.0,""bar_sigma"":0.0,""assumptions"":{""style"":"""",""road"":"""",""speed_kmh"":0,""lane_quality"":""""},""rationale"":""""} " + ...
%     " Ensure values are from the allowed sets and remember these are initial priors, not ground truth."
% ];

prompt = [
    "You are an expert inferring initial controller priors from a short user preference." + ...
    " Return STRICT JSON with keys: e_max, mu_0, sigma_0, bar_sigma, init_v, assumptions, rationale." + ...
    " Meanings:" + ...
    " - e_max: maximum lane tracking error tolerance. Larger for more aggressive turns/risk; smaller for more conservative/precise." + ...
    " - mu_0: initial prior for road–tire friction (icy small, normal medium, dry large)." + ...
    " - sigma_0: uncertainty (std^2) of the friction prior; larger if the user sounds unsure or contradictory." + ...
    " - bar_sigma: confidence of the estimator on its measurements; increase if not sure if estimator is good." + ...
    " - init_v: initial velocity of the vehicle. large for aggressive, small for conservative." + ...
    " Policy:" + ...
    " - If the user uses vague words (""seems"", ""maybe"", ""not sure"", ""probably""), or does not sound certain, use sigma_0=0.3." + ...
    " - Keep bar_sigma=0.3" + ...
    " - Only change e_max with explicit user cues about aggressiveness or precision." + ...
    " Valid discrete ranges:" + ...
    " - e_max ∈ {3,5,10}; mu_0 ∈ {0.3,0.5,0.9}; sigma_0 ∈ {0.05,0.3}; bar_sigma ∈ {0.05,0.3}; init_v ∈ {10,20}." + ...
    " Output ONLY JSON in this exact shape: " + ...
    " {""e_max"":0,""mu_0"":0.0,""sigma_0"":0.0,""bar_sigma"":0.0,""init_v"":0,""assumptions"":{""style"":"""",""road"":"""",""speed_kmh"":0,""lane_quality"":""""},""rationale"":""""} " + ...
    " Ensure values are from the allowed sets and remember these are initial priors, not ground truth."
    ];
% "Valid ranges: e_max ∈ {3,5,10}; mu_0 ∈ {0.2,0.9}; sigma_0 ∈ [0.01,0.5]; bar_sigma ∈ [0.01,0.5]. " + ...

if contains(p.Results.ModelName, "gpt", "IgnoreCase", true)
    % --- GPT branch ---
    fprintf("working with "+ p.Results.ModelName);
    model = openAIChat(prompt,ModelName=p.Results.ModelName);
    resp = generate(model, user_input, Temperature=0.2, MaxNumTokens=300);

elseif contains(p.Results.ModelName, "gemini", "IgnoreCase", true)
    % --- Gemini branch ---
    fprintf("working with "+ p.Results.ModelName)
    apiKey = getenv('X-goog-api-key');
    user_prompt = sprintf("\nUser instruction: %s", user_input);
    url = "https://generativelanguage.googleapis.com/v1beta/models/" + p.Results.ModelName + ":generateContent";
    fullUrl = url + "?key=" + apiKey;

    full_prompt = prompt + user_prompt;

    body = struct("contents", struct("role", "user", "parts", struct("text", full_prompt)), ...
        "generationConfig", struct( ...
        "temperature", 0.2, ...
        "maxOutputTokens", 8192, ...
        "candidateCount", 1 ...
        ) ...
        );

    opts = weboptions( ...
        "HeaderFields", {'x-goog-api-key', apiKey}, ...
        "MediaType","application/json", ...
        "Timeout",60 ...
        );

    % === Call Gemini ===
    resp = webwrite(fullUrl, body, opts);

    % === Parse returned JSON ===
    resp = resp.candidates(1).content.parts(1).text;

elseif contains(p.Results.ModelName, "deepseek", "IgnoreCase", true)
    % --- Deepseek branch ---
    fprintf("working with "+ p.Results.ModelName + "\n")
    deepseekApiKey = '...';
    apiUrl = 'https://api.deepseek.com/chat/completions';

    % Prepare the request headers
    headers = {'Authorization', ['Bearer ' deepseekApiKey]; ...
        'Content-Type', 'application/json'};

    % Prepare the messages with system prompt and user prompt
    messages = [
        struct('role', 'system', 'content', prompt);
        struct('role', 'user', 'content', user_input)
        ];

    % Create request body
    requestBody = struct(...
        'model', p.Results.ModelName, ...
        'messages', messages, ...
        'stream', false, ...
        'max_tokens', 2048 ...
        );

    % Convert to JSON
    jsonBody = jsonencode(requestBody);

    % Make API call
    options = weboptions(...
        'RequestMethod', 'post', ...
        'HeaderFields', headers, ...
        'MediaType', 'application/json', ...
        'Timeout', 60 ...
        );


    apiResponse = webwrite(apiUrl, jsonBody, options);
    resp = apiResponse.choices(1).message.content;

    % % Extract the response content
    % if isfield(apiResponse, 'choices') && ~isempty(apiResponse.choices)
    %     resp = apiResponse.choices(1).message.content;
    % else
    %     resp = 'Error: No response from DeepSeek API';
    % end


else
    error("Unknown model name: %s", p.Results.ModelName);
end

% with Large Language Models (LLMs) with MATLAB add-on
% model = openAIChat(prompt,ModelName="gpt-4o-mini");
% resp = generate(model, user_input, Temperature=0.2, MaxNumTokens=300);


% % without add-on
% url  = "https://api.openai.com/v1/responses";
% opts = weboptions( ...
%     "HeaderFields", {'Authorization',['Bearer ' char(apiKey)]}, ...
%     "MediaType","application/json", ...
%     "Timeout",60);
%
%
% body = struct( ...
%     "model","gpt-4o-mini", ...
%     "input",prompt + user_input ...
% );
%
% resp = webwrite(url, body, opts);
% resp = resp.output.content.text;

% display result
resp = regexprep(resp, '```[a-zA-Z]*', '');  % remove ```json, ```JSON, etc.
resp = strrep(resp, '```', '');              % remove closing ```
resp = strtrim(resp);
expr = '{.*}';
match = regexp(resp, expr, 'match');
if ~isempty(match)
    resp = match{1};
end
S = jsondecode(resp);
end